# Baza embedowanych ksiƒÖ≈ºek EPUB - Semantic Search

## Opis systemu
Lokalna baza do semantycznego przeszukiwania 209 ksiƒÖ≈ºek EPUB z biblioteki Calibre. Dzia≈Ça bezpo≈õrednio w Claude Code przez Python CLI.

## Lokalizacja
- **Folder ksiƒÖ≈ºek**: `/Users/robert/Books/books/`
- **Liczba ksiƒÖ≈ºek**: 209 plik√≥w EPUB
- **Struktura**: Katalog Calibre (Autor/Tytu≈Ç/plik.epub)

## Architektura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude Code (Terminal)                 ‚îÇ
‚îÇ  Komendy: search, index, update, status ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python CLI (cli.py)                    ‚îÇ
‚îÇ  - search_books("stoicyzm")             ‚îÇ
‚îÇ  - index_library()                      ‚îÇ
‚îÇ  - update_new_books()                   ‚îÇ
‚îÇ  - check_status()                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇebooklib ‚îÇ    ‚îÇ OpenAI API   ‚îÇ
‚îÇ(czyta   ‚îÇ    ‚îÇ(embeddings:  ‚îÇ
‚îÇ EPUB)   ‚îÇ    ‚îÇtext-embed-3) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ChromaDB (embedded)        ‚îÇ
‚îÇ  ./rag-system/data/chromadb ‚îÇ
‚îÇ  - Embeddingi tekst√≥w       ‚îÇ
‚îÇ  - Metadata (autor, tytu≈Ç)  ‚îÇ
‚îÇ  - Status indeksowania      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Technologie

### Core components:
- **ebooklib** - bezpo≈õrednie czytanie EPUB (bez MCP servera)
- **OpenAI API** - `text-embedding-3-small` dla generowania embedding√≥w
  - Multilingual (Polski + Angielski)
  - Szybkie (~2-3 sek/ksiƒÖ≈ºka)
  - Koszt: ~$0.50-1.00 jednorazowo dla 209 ksiƒÖ≈ºek
- **ChromaDB** - embedded vector database (jak SQLite, bez servera)
- **Python CLI** - prosty interface w terminalu

### Chunking strategy (2-poziomowa):

**Poziom 1: Rozdzia≈Çy**
- Ca≈Çy tekst rozdzia≈Çu (2000-5000 token√≥w)
- Metadata: `chunk_type: "chapter"`
- Use case: "Znajd≈∫ rozdzia≈Ç o X"

**Poziom 2: Paragrafy**
- Fragmenty ~300-500 token√≥w
- Metadata: `chunk_type: "paragraph"`
- Use case: "Znajd≈∫ konkretny cytat/fragment o X"

Przy wyszukiwaniu mo≈ºna wybraƒá poziom lub oba jednocze≈õnie.

## Struktura projektu

```
books/
‚îú‚îÄ‚îÄ claude.md                    # Ten dokument
‚îú‚îÄ‚îÄ .env                        # OPENAI_API_KEY=sk-...
‚îú‚îÄ‚îÄ rag-system/                  # System RAG
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                  # Main CLI interface
‚îÇ   ‚îú‚îÄ‚îÄ indexer.py              # Skanuje EPUB ‚Üí chunki ‚Üí embeddingi
‚îÇ   ‚îú‚îÄ‚îÄ searcher.py             # Semantic search
‚îÇ   ‚îú‚îÄ‚îÄ epub_parser.py          # ebooklib wrapper
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Konfiguracja
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ chromadb/           # Embedded vector database
‚îÇ       ‚îî‚îÄ‚îÄ index_status.json   # Stan indeksowania
‚îî‚îÄ‚îÄ [Calibre folders...]        # IstniejƒÖce ksiƒÖ≈ºki EPUB
```

## U≈ºytkowanie

### Setup (jednorazowo):
```bash
cd /Users/robert/Books/books/rag-system
pip install -r requirements.txt
export OPENAI_API_KEY="sk-..."  # Lub w pliku .env
python cli.py init
```

### Indeksowanie (pierwsza konfiguracja):
```bash
python cli.py index --full
# Skanuje wszystkie 209 ksiƒÖ≈ºek EPUB
# Ekstraktuje rozdzia≈Çy + paragrafy
# Generuje embeddingi przez OpenAI API
# Zapisuje w ChromaDB
# Czas: ~15-30 minut
```

### Wyszukiwanie:
```bash
# Szukaj w ca≈Çej bibliotece
python cli.py search "ksiƒÖ≈ºki o stoicyzmie i medytacji"

# Szukaj tylko w rozdzia≈Çach
python cli.py search "stoicism" --level chapter

# Szukaj tylko w paragrafach
python cli.py search "stoicism" --level paragraph

# Filtruj po autorze
python cli.py search "meditation" --author "Marcus Aurelius"

# Wiƒôcej wynik√≥w
python cli.py search "economics" --top 20
```

Przyk≈Çadowy output:
```
üìñ Found 8 results:

1. Meditations - Marcus Aurelius (Chapter 2)
   "When you wake up in the morning, tell yourself:
   the people I deal with today will be meddling..."
   Similarity: 0.89

2. The Power of Now - Eckhart Tolle (Chapter 5)
   "The essence of meditation is to be fully present..."
   Similarity: 0.84

3. ...
```

### Aktualizacja (dodanie nowych ksiƒÖ≈ºek):
```bash
python cli.py update
# Automatycznie znajduje nowe EPUB
# Indeksuje tylko nowe ksiƒÖ≈ºki
# Aktualizuje index_status.json
```

### Status:
```bash
python cli.py status

# Output:
# ‚úÖ 209/209 books indexed
# üìä 2,456 chapters, 18,942 paragraphs
# üíæ Database size: 145MB
# üïê Last update: 2 hours ago
```

### Auto-check przy starcie:
System automatycznie sprawdza nowe ksiƒÖ≈ºki przy ka≈ºdym uruchomieniu i informuje:
```bash
python cli.py search "..."
# üìö Found 3 new books. Run 'python cli.py update' to index them.
#
# üìñ Searching...
```

## Plik index_status.json

Przechowuje informacje o zindeksowanych ksiƒÖ≈ºkach:
```json
{
  "indexed_books": {
    "Marcus Aurelius/Meditations": {
      "indexed_at": "2025-10-27T10:30:00",
      "chapters": 12,
      "paragraphs": 156,
      "file_hash": "abc123...",
      "file_path": "./Marcus Aurelius/Meditations/..."
    }
  },
  "total_indexed": 209,
  "last_update": "2025-10-27T10:30:00"
}
```

## ChromaDB - embedded database

ChromaDB dzia≈Ça jak SQLite - ≈ºadnych serwer√≥w:
- Automatyczne tworzenie przy pierwszym u≈ºyciu
- Lokalne przechowywanie w `./data/chromadb/`
- Szybkie wyszukiwanie (milisekundy dla 209 ksiƒÖ≈ºek)
- Persistence automatyczny

```python
# Tak to dzia≈Ça pod spodem:
import chromadb
client = chromadb.PersistentClient(path="./data/chromadb")
collection = client.get_or_create_collection("books")
```

## Aktualizacja bazy

Kiedy dodajesz nowe ksiƒÖ≈ºki do katalogu Calibre:
```bash
# System automatycznie wykryje nowe pliki
python cli.py update

# Lub pe≈Çne ponowne indeksowanie
python cli.py index --full --force
```

## Koszty OpenAI API

**text-embedding-3-small**:
- Cena: $0.02 za 1M token√≥w
- Szacunek dla 209 ksiƒÖ≈ºek: ~$0.50-1.00 jednorazowo
- Aktualizacje (nowe ksiƒÖ≈ºki): kilka cent√≥w per ksiƒÖ≈ºka

## Dependencies (requirements.txt)

```
chromadb>=0.4.0
ebooklib>=0.18
beautifulsoup4>=4.12.0
html2text>=2020.1.16
openai>=1.0.0
python-dotenv>=1.0.0
click>=8.1.0
tqdm>=4.66.0
pydantic>=2.0.0
```

## Wsparcie jƒôzykowe

System obs≈Çuguje:
- üáµüá± Polski
- üá¨üáß Angielski
- Multilingual queries (mo≈ºna mieszaƒá jƒôzyki w zapytaniu)

OpenAI embeddings sƒÖ multilingual out-of-the-box.

## Komendy CLI - podsumowanie

```bash
python cli.py init              # Setup poczƒÖtkowy
python cli.py index --full      # Indeksuj wszystkie ksiƒÖ≈ºki
python cli.py search "query"    # Wyszukiwanie semantyczne
python cli.py update            # Dodaj nowe ksiƒÖ≈ºki
python cli.py status            # Sprawd≈∫ status bazy
python cli.py reindex "book"    # Reindeksuj konkretnƒÖ ksiƒÖ≈ºkƒô
python cli.py clear             # Wyczy≈õƒá ca≈ÇƒÖ bazƒô (ostro≈ºnie!)
```

## Jak to dzia≈Ça pod spodem

1. **Indexing**:
   - ebooklib czyta EPUB ‚Üí ekstraktuje HTML
   - BeautifulSoup parsuje strukturƒô (rozdzia≈Çy)
   - Tekst dzielony na chunks (rozdzia≈Çy + paragrafy)
   - OpenAI API generuje embeddingi (1536-dimensional vectors)
   - ChromaDB zapisuje: embedding + metadata + text

2. **Searching**:
   - Query ‚Üí OpenAI embedding
   - ChromaDB: cosine similarity search
   - Top-k najbardziej podobnych chunks
   - Return z metadatami (ksiƒÖ≈ºka, autor, rozdzia≈Ç)

3. **Update**:
   - Scan Calibre folder
   - Por√≥wnaj z index_status.json
   - Indeksuj tylko nowe/zmienione pliki

## Status implementacji (2025-10-28)

### ‚úÖ Zrealizowane (v1.0)
1. ‚úÖ Setup struktury projektu
2. ‚úÖ Implementacja `epub_parser.py` (ebooklib wrapper)
3. ‚úÖ Implementacja `indexer.py` (chunking + OpenAI API + adaptive chunking)
4. ‚úÖ Implementacja `searcher.py` (ChromaDB queries)
5. ‚úÖ Implementacja `cli.py` (user interface)
6. ‚úÖ Testing i optymalizacja
7. ‚úÖ Full indexing - 191/209 ksiƒÖ≈ºek (91% sukcesu)
8. ‚úÖ Publikacja na GitHub: https://github.com/iamrobmat/local-ebook-ai-chat-library

**Obecna funkcjonalno≈õƒá: Semantic Search**
- User Query ‚Üí Vector Search ‚Üí Lista relevantnych fragment√≥w z ksiƒÖ≈ºek
- CLI: `python cli.py search "pytanie"`

---

## üöÄ Nastƒôpne kroki rozwoju (v2.0)

### Priorytet 1: RAG - Generowanie odpowiedzi (GPT-4)

**Cel:** Transformacja z "semantic search" na "AI assistant" kt√≥ry odpowiada na pytania na bazie ksiƒÖ≈ºek.

**Obecny flow:**
```
User Query ‚Üí Semantic Search ‚Üí Lista fragment√≥w
```

**Docelowy flow (RAG):**
```
User Query ‚Üí Semantic Search ‚Üí GPT-4 + Context ‚Üí Odpowied≈∫ + ≈πr√≥d≈Ça
```

**Do zaimplementowania:**

#### 1. Modu≈Ç `answerer.py`
```python
class BookAnswerer:
    """Generates answers using GPT-4 based on book passages."""

    def generate_answer(self, question: str, passages: List[SearchResult]) -> Answer:
        """
        Generate answer using GPT-4 with retrieved context.

        Args:
            question: User's question
            passages: Top-k relevant passages from semantic search

        Returns:
            Answer object with text, sources, and citations
        """
```

**Funkcje:**
- Prompt engineering dla GPT-4
- Formatowanie kontekstu (top 5-10 passages)
- Generowanie odpowiedzi z cytatami
- Tracking ≈∫r√≥de≈Ç (ksiƒÖ≈ºka, autor, rozdzia≈Ç, strona)
- Error handling i fallbacks

#### 2. CLI komenda `ask`
```bash
python cli.py ask "What is stoicism?"

ü§ñ Answer:
Stoicism is an ancient Greek philosophy founded by Zeno of Citium around
300 BCE. The philosophy emphasizes virtue, wisdom, and living in accordance
with nature. According to Marcus Aurelius, the key principle is accepting
what we cannot control while focusing on our own thoughts and actions.

üìö Sources:
1. "Meditations" - Marcus Aurelius (Book 2, Chapter 1)
   "You have power over your mind - not outside events..."

2. "The Daily Stoic" - Ryan Holiday (Introduction)
   "Stoicism teaches us to focus only on what we can control..."
```

**Koszty:** ~$0.01-0.02 per pytanie (GPT-4o-mini) lub ~$0.05-0.10 (GPT-4)

#### 3. Konfiguracja w `config.py`
```python
class RAGConfig(BaseModel):
    llm_model: str = "gpt-4o-mini"  # lub "gpt-4"
    max_context_passages: int = 10
    temperature: float = 0.3
    max_tokens: int = 1000
    include_citations: bool = True
```

**Szacowany czas implementacji:** 2-3 godziny

---

### Priorytet 2: CLI Chat Interface - Interaktywna konwersacja

**Cel:** Wieloturowa konwersacja z AI assistant, z historiƒÖ i kontekstem.

#### Biblioteki CLI Chat (badania 2025-01)

**NAJLEPSZA OPCJA: `prompt_toolkit` + `asyncio` + OpenAI**

Sprawdzony stack z produkcyjnych implementacji:
- ‚úÖ **prompt_toolkit** - profesjonalny terminal UI
- ‚úÖ **asyncio** - async handling dla streaming responses
- ‚úÖ **OpenAI AsyncClient** - async API calls
- ‚úÖ Historia komend (‚Üë/‚Üì), multi-line input, Vim/Emacs modes
- ‚úÖ Slash commands (`/help`, `/clear`, `/save`)
- ‚úÖ Session persistence (JSON files)

**≈πr√≥d≈Ço:** [Building a Terminal LLM Chat App](https://recruit.gmo.jp/engineer/jisedai/blog/building-a-terminal-llm-chat-app-with-python-asyncio/) (GMO Engineering, 2025)

**Instalacja:**
```bash
pip install prompt-toolkit openai  # asyncio jest built-in
```

**Alternatywne biblioteki:**

**1. `rich` (dodatkowe formatowanie)**
- ‚úÖ Kolorowy output, markdown rendering
- ‚úÖ Progress bars, tables, panels
- ‚úÖ Syntax highlighting
- Mo≈ºna kombinowaƒá z prompt_toolkit
```bash
pip install rich
```

**2. `simpleaichat`** (gotowiec)
- ‚úÖ One-liner chat interface
- ‚úÖ Minimal code complexity
- ‚ùå Mniej kontroli nad UI
```bash
pip install simpleaichat
```

**3. `Chainlit` / `Gradio`** (Web UI)
- ‚úÖ Szybki prototyp z GUI
- ‚úÖ Built-in chat components
- ‚ùå Nie jest CLI (wymaga browsera)

**4. `cmd` (built-in Python)**
- ‚úÖ Zero dependencies
- ‚úÖ Prosty REPL
- ‚ùå Brak zaawansowanych features
- ‚ùå Nie obs≈Çuguje async

**Rekomendacja ko≈Ñcowa:** `prompt_toolkit` + `asyncio` + `rich` (formatowanie)

### Gotowe przyk≈Çady do inspiracji:

1. **GeminiAI CLI** - https://github.com/notsopreety/geminiai
   - Session management, dynamic prompts, streaming

2. **OpenAI Terminal Chatbot** - https://www.digitalocean.com/community/tutorials/openai-terminal-chatbot
   - Tutorial DigitalOcean z przyk≈Çadami

3. **ai-cli-chat** (PyPI) - https://pypi.org/project/ai-cli-chat/
   - Multi-model support, round-table discussions

#### Implementacja chat interface

```bash
python cli.py chat

‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ  üìö Local eBook AI Chat (RAG Mode)     ‚îÇ
‚îÇ  Type 'help' for commands, 'exit' to quit ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

You: What is stoicism?

ü§ñ Assistant: [streaming response...]
Stoicism is an ancient Greek philosophy...

   üìñ Sources: Meditations (Marcus Aurelius), The Daily Stoic (Ryan Holiday)

You: How do stoics deal with adversity?

ü§ñ Assistant: [continuing from context...]
Building on the previous answer about stoicism, stoics view adversity as...

You: /sources
üìö Conversation sources (2 questions):
  1. Meditations - Marcus Aurelius
  2. The Daily Stoic - Ryan Holiday
  3. Enchiridion - Epictetus

You: /clear
‚úì Conversation history cleared.

You: exit
Goodbye! üëã
```

**Features:**
- Wieloturowa konwersacja z pamiƒôciƒÖ
- Historia poprzednich pyta≈Ñ jako kontekst
- Komendy specjalne:
  - `/help` - pomoc
  - `/sources` - poka≈º ≈∫r√≥d≈Ça z konwersacji
  - `/clear` - wyczy≈õƒá historiƒô
  - `/save` - zapisz konwersacjƒô
  - `/exit` lub `exit` - wyjd≈∫
- Streaming responses (opcjonalnie)
- Rich formatting (kolory, panele, markdown)

**Architektura techniczna (wg GMO Engineering):**

```python
# chat_interface.py

import asyncio
from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from openai import AsyncOpenAI
from pathlib import Path

class ChatInterface:
    """Interactive chat interface using prompt_toolkit + asyncio."""

    def __init__(self, answerer: BookAnswerer, config: SystemConfig):
        self.answerer = answerer
        self.config = config

        # Conversation history: {session_id: [messages]}
        self.conversations = {}
        self.current_session = "default"

        # Prompt toolkit session with history
        history_file = Path.home() / ".local/share/ebook-chat/history.txt"
        history_file.parent.mkdir(parents=True, exist_ok=True)
        self.session = PromptSession(history=FileHistory(str(history_file)))

        # OpenAI async client
        self.client = AsyncOpenAI(api_key=config.openai.api_key)

    async def send_message(self, user_input: str) -> str:
        """Send message to LLM with conversation context."""

        # Get conversation history
        if self.current_session not in self.conversations:
            self.conversations[self.current_session] = []

        messages = self.conversations[self.current_session]

        # 1. Semantic search for context (from BookSearcher)
        search_results = await self.answerer.search_async(user_input)

        # 2. Format context from books
        context = self._format_context(search_results)

        # 3. Build messages with context
        messages.append({
            "role": "user",
            "content": f"Context from books:\n{context}\n\nQuestion: {user_input}"
        })

        # 4. Stream response from GPT-4
        full_response = ""
        async for chunk in await self.client.chat.completions.create(
            model=self.config.rag.llm_model,
            messages=messages,
            stream=True
        ):
            if chunk.choices[0].delta.content:
                text = chunk.choices[0].delta.content
                full_response += text
                print(text, end="", flush=True)  # Stream to terminal

        # 5. Save assistant response
        messages.append({"role": "assistant", "content": full_response})

        return full_response

    async def run(self):
        """Start interactive chat loop."""
        print("üìö Local eBook AI Chat")
        print("Type '/help' for commands, 'exit' to quit.\n")

        shutdown_event = asyncio.Event()

        # Handle Ctrl+C gracefully
        import signal
        signal.signal(signal.SIGINT, lambda s, f: shutdown_event.set())

        try:
            while not shutdown_event.is_set():
                # Get user input (supports multi-line with prompt_toolkit)
                try:
                    user_input = await self.session.prompt_async("You: ")
                except KeyboardInterrupt:
                    break

                if not user_input.strip():
                    continue

                # Handle slash commands
                if user_input.startswith("/"):
                    await self._handle_command(user_input)
                    continue

                # Exit command
                if user_input.lower() in ["exit", "quit"]:
                    break

                # Send to LLM
                print("ü§ñ Assistant: ", end="")
                await self.send_message(user_input)
                print("\n")

        finally:
            # Save session on exit
            self._save_sessions()
            print("\nGoodbye! üëã")

    async def _handle_command(self, command: str):
        """Handle special slash commands."""
        parts = command.split()
        cmd = parts[0].lower()

        if cmd == "/help":
            print("""
Available commands:
  /help              - Show this help
  /clear             - Clear conversation history
  /sources           - Show sources used in conversation
  /save [filename]   - Save conversation to file
  /sessions          - List all sessions
  /switch [session]  - Switch to different session
  exit               - Exit chat
            """)

        elif cmd == "/clear":
            self.conversations[self.current_session] = []
            print("‚úì Conversation history cleared.")

        elif cmd == "/sources":
            # Show all book sources from current conversation
            sources = self._extract_sources()
            print(f"\nüìö Sources used in this conversation:")
            for i, source in enumerate(sources, 1):
                print(f"  {i}. {source}")

        elif cmd == "/save":
            filename = parts[1] if len(parts) > 1 else "conversation.md"
            self._save_conversation(filename)
            print(f"‚úì Saved to {filename}")

        elif cmd == "/sessions":
            print("\nAvailable sessions:")
            for session_id in self.conversations:
                msg_count = len(self.conversations[session_id])
                marker = "‚Üí" if session_id == self.current_session else " "
                print(f"  {marker} {session_id} ({msg_count} messages)")

        else:
            print(f"Unknown command: {cmd}. Type /help for available commands.")

    def _save_sessions(self):
        """Persist sessions to JSON file."""
        sessions_file = Path.home() / ".local/share/ebook-chat/sessions.json"
        sessions_file.parent.mkdir(parents=True, exist_ok=True)

        import json
        with open(sessions_file, 'w') as f:
            json.dump(self.conversations, f, indent=2)
```

**Integracja z CLI (`cli.py`):**

```python
@cli.command()
def chat():
    """Start interactive chat with your book library (RAG mode)."""
    try:
        from chat_interface import ChatInterface

        # Initialize components
        config = get_config()
        answerer = BookAnswerer(config)
        interface = ChatInterface(answerer, config)

        # Run async event loop
        asyncio.run(interface.run())

    except KeyboardInterrupt:
        print("\n\nInterrupted. Goodbye!")
    except Exception as e:
        print(f"Error: {e}")
        raise
```

**Szacowany czas implementacji:** 2-3 godziny (z async + streaming)

---

### Priorytet 3: Rozszerzenia (opcjonalne)

#### 3.1 Streamlit/Gradio UI (Web Interface)
```bash
python cli.py ui
# ‚Üí Otwiera przeglƒÖdarkƒô z GUI
```
- Drag & drop EPUB upload
- Wizualna historia konwersacji
- PodglƒÖd ≈∫r√≥de≈Ç z highlight'ami
- **Czas:** 3-4 godziny

#### 3.2 Wsparcie dla innych format√≥w
- PDF (PyPDF2/pdfplumber)
- MOBI (mobi)
- TXT (plain text)
- **Czas:** 2-3 godziny per format

#### 3.3 Export konwersacji
```bash
python cli.py chat --export conversation.md
```
- Export do Markdown, JSON, PDF
- **Czas:** 1 godzina

#### 3.4 Voice input (opcjonalnie)
- Whisper API dla voice-to-text
- **Czas:** 2-3 godziny

---

## Architektura RAG (v2.0)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLI: chat / ask                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BookAnswerer (answerer.py)             ‚îÇ
‚îÇ  - Prompt engineering                    ‚îÇ
‚îÇ  - Context management                    ‚îÇ
‚îÇ  - Citation formatting                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Searcher ‚îÇ    ‚îÇ GPT-4 API  ‚îÇ
‚îÇ (exist.) ‚îÇ    ‚îÇ (new)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì                 ‚Üì
  ChromaDB      [Answer + Citations]
```

### Flow pojedynczego pytania (ask):
1. User: `python cli.py ask "What is stoicism?"`
2. Semantic Search ‚Üí Top 10 passages
3. Format context for GPT-4
4. GPT-4 generates answer + citations
5. Display formatted answer + sources

### Flow konwersacji (chat):
1. User enters chat mode
2. Loop:
   - User types question
   - System adds question to history
   - Semantic search (with conversation context)
   - GPT-4 generates answer (aware of previous turns)
   - Display answer
   - Update history
3. Special commands (/sources, /clear, etc.)
4. Exit on 'exit' or Ctrl+C

---

## Koszty rozszerzenia (v2.0)

**RAG z GPT-4o-mini (rekomendowane):**
- Embedding (ju≈º policzone): ~$0.50-1.00 jednorazowo
- Answer generation: ~$0.01-0.02 per pytanie
- 100 pyta≈Ñ: ~$1-2
- 1000 pyta≈Ñ: ~$10-20

**RAG z GPT-4 (premium):**
- Answer generation: ~$0.05-0.10 per pytanie
- 100 pyta≈Ñ: ~$5-10

**Rekomendacja:** ZaczƒÖƒá od GPT-4o-mini (ta≈Ñszy, szybszy, wystarczajƒÖco dobry)

---

## Dependencies do dodania (v2.0)

**requirements.txt - additions:**
```
# RAG
openai>=1.0.0  # ju≈º mamy, u≈ºywamy r√≥wnie≈º dla GPT-4

# CLI Chat Interface
prompt-toolkit>=3.0.0   # Interactive REPL
rich>=13.0.0            # Beautiful terminal output
pyperclip>=1.8.0        # Clipboard support (optional)

# Optional UI
streamlit>=1.20.0       # Web UI (optional)
gradio>=3.0.0          # Alternative Web UI (optional)
```

---

## Timeline implementacji

### Sprint 1: RAG Core (1 dzie≈Ñ)
- [ ] `answerer.py` - modu≈Ç RAG
- [ ] `cli.py ask` - komenda pojedynczego pytania
- [ ] Prompt engineering i testing
- [ ] Dokumentacja u≈ºycia

### Sprint 2: Chat Interface (1 dzie≈Ñ)
- [ ] `chat_interface.py` - interaktywny chat
- [ ] Integracja z prompt_toolkit + rich
- [ ] Historia konwersacji
- [ ] Komendy specjalne
- [ ] Testing UX

### Sprint 3: Polish & Docs (0.5 dnia)
- [ ] README update z przyk≈Çadami RAG
- [ ] CHANGELOG update
- [ ] Testing end-to-end
- [ ] GitHub release v2.0

**Total: ~2-3 dni pracy**

---

## Pytania do rozstrzygniƒôcia

1. **Model GPT:** GPT-4o-mini (ta≈Ñszy) czy GPT-4 (lepszy)?
2. **UI:** Tylko CLI czy te≈º Streamlit/Gradio?
3. **Formaty:** Zostaƒá przy EPUB czy rozszerzyƒá o PDF?
4. **Historia:** Zapisywaƒá konwersacje do pliku czy tylko in-memory?
5. **Streaming:** Streamowaƒá odpowiedzi GPT (jak ChatGPT) czy ca≈Ço≈õƒá naraz?

---

## Niezale≈ºno≈õƒá od narzƒôdzi

**WA≈ªNE:** Projekt jest w 100% niezale≈ºny od:
- ‚ùå Claude Code
- ‚ùå Jakiegokolwiek IDE
- ‚ùå Specjalnych narzƒôdzi

**Wymaga tylko:**
- ‚úÖ Python 3.8+
- ‚úÖ pip
- ‚úÖ OpenAI API key

**Dzia≈Ça na:**
- ‚úÖ Linux
- ‚úÖ macOS
- ‚úÖ Windows
- ‚úÖ Dowolnym terminalu

---

## üöÄ Nastƒôpne kroki (v2.2)

1. **PyMuPDF parser** - PDF + MOBI + inne formaty (zamiast tylko EPUB) - 60x szybszy, `pip install PyMuPDF`
2. **Zapis rozmowy** - `/save filename.md` w chat
3. **Schowek** - `/copy` dla ostatniej odpowiedzi
4. **Pasek postƒôpu** - feedback podczas generowania (streaming GPT lub etapy)
5. **Kolorki** - `rich` library dla ≈Çadniejszego terminala
